//tag::ref-doc[]
:image-root: https://raw.githubusercontent.com/spring-cloud-stream-app-starters/tensorflow/master/images
= Pose Estimation Service

[cols=2*]
|===
| Real-time, multi-person Pose Estimation service for detecting human figures in images and video. Used for determining where different body parts
  are located in an image an how are they spatially relate to each other.

  Processor is based on the https://arxiv.org/pdf/1611.08050.pdf[Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields],
   https://github.com/CMU-Perceptual-Computing-Lab/openpose[OpenPose] and https://github.com/ildoonet/tf-pose-estimation[tf-pose-estimation].
| image:./src/test/resources/doc/webcamPoseEstimation.gif[]
|===


The Pose Estimation Service works with a pre-trained Tensorflow model, build with the https://github.com/ildoonet/tf-pose-estimation[tf-pose-estimation] project.
The inference of this model produces auxiliary data structures such a heatmaps with predictions about the parts locations in the image. The post-processing required
for selecting the right body parts and grouping them into poses are implemented by the processor using greedy algorithms.

There are two pre-trained models available out of the box:

*  `http://dl.bintray.com/big-data/generic/2018-30-05-mobilenet_thin_graph_opt.pb` (default) - fast but less accurate
*  `http://dl.bintray.com/big-data/generic/2018-05-14-cmu-graph_opt.pb` - accurate but slower

Service's input is an image byte array or array of byte arrays for batch images and the output is a JSON message and optionally an image with annotated body poses.
The output JSON format looks like:

```json
[
    {
        "limbs": [ {"score": 8.4396105, "from": { "type": "lShoulder", "y": 56, "x": 160 }, "to": { "type": "lEar", "y": 24, "x": 152 } },
                   { "score": 10.145516, "from": { "type": "neck", "y": 56, "x": 144 }, "to": { "type": "rShoulder", "y": 56, "x": 128 } },
                   { "score": 9.970467, "from": { "type": "neck", "y": 56, "x": 144 }, "to": { "type": "lShoulder", "y": 56, "x": 160 } } ]
    },
    {
        "limbs": [ {"score": 7.85779, "from": { "type": "neck", "y": 48, "x": 328 }, "to": { "type": "rHip", "y": 128, "x": 328 } },
                   {"score": 6.8949876, "from": { "type": "neck", "y": 48, "x": 328 }, "to": { "type": "lHip", "y": 128, "x": 304 } } ]
   }
]
```

Every entry in the array  represents a single body posture found on the image. Bodies are composed of Parts connected by Limbs represented by the  *limbs* collection.
Every *Limb* instance has a PAF confidence score and the `from` and `to` parts it connects. The *Part* instances have a `type` and coordinates.

NOTE: Output image annotated with body pose skeletons
When the `tensorflow.mode=header` property is set the JSON metadata passed inside the output message header while the payload
contains a copy of the input image. If the `tensorflow.pose.estimation.drawPoses=true` is set the copied input image is
augmented with the poses described in the JSON metadata.

//end::ref-doc[]
== Build


```
$ mvn clean install
```

== Usage

Add the `pose-estimation` dependency to your pom:

```xml
    <parent>
        <groupId>io.mindmodel.services</groupId>
        <artifactId>pose-estimation</artifactId>
        <version>1.0.0-SNAPSHOT</version>
    </parent>
```
(_Use the latest version available_)


Then create a `PoseEstimationService` with one of the pre-trained models and detect the poses input image.

```java
    ResourceLoader resourceLoader = new DefaultResourceLoader();

    try (InputStream is = resourceLoader.getResource("classpath:/images/tourists.jpg").getInputStream()) {

        byte[] inputImage = StreamUtils.copyToByteArray(is);

        Resource model = resourceLoader.getResource("https://dl.bintray.com/big-data/generic/2018-05-14-cmu-graph_opt.pb");
        PoseEstimationService poseEstimationService = new PoseEstimationService(model, true);

        List<Body> bodies = poseEstimationService.detect(inputImage);
        System.out.println("Body List: " + bodies);

        ...
    }
```
You can use the `JsonMapperFunction` function to turn the `Body` list into JSON objects:

```java

        String bodiesJson = new JsonMapperFunction().apply(bodies);
        System.out.println("Pose JSON: " + bodiesJson);
```

Or the `PoseEstimateImageAugmenter` function to draw the detected body skeletons on top of the input image:

```java
        byte[] augmentedImage = new PoseEstimateImageAugmenter().apply(inputImage, bodies);
        IOUtils.write(augmentedImage, new FileOutputStream("./pose-estimation/target/tourists-augmented.jpg"));
```

image:./src/test/resources/doc/tourists-augmented.jpg[]

